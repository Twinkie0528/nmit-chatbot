import nltk
import os
import pickle
import numpy as np
import json
import random

from keras.models import load_model
from nltk.tokenize import wordpunct_tokenize  # üÜï
from nltk.stem import WordNetLemmatizer
from pymongo import MongoClient
from datetime import datetime

# ‚úÖ nltk_data path-—É—É–¥—ã–≥ –∑–∞–∞–∂ ”©–≥–Ω”© (punkt, wordnet –∞–ª—å –∞–ª—å –Ω—å)
nltk.data.path.append(os.path.join(os.path.dirname(__file__), "nltk_data"))
nltk.data.path.append(os.path.join(os.path.dirname(__file__), "nltk_data", "corpora"))

# üßπ –õ–µ–º–º–∞—Ç–∞–π–∑–µ—Ä
lemmatizer = WordNetLemmatizer()

# üíæ MongoDB —Ö–æ–ª–±–æ–ª—Ç
try:
    client = MongoClient("mongodb://localhost:27017/")
    db = client.chatbot_db
    questions = db.questions
except:
    client = None
    questions = None

def log_user_question(user_input, intent=None):
    if questions:
        questions.insert_one({
            "question": user_input,
            "intent": intent,
            "timestamp": datetime.utcnow()
        })

# üß† –ó–∞–≥–≤–∞—Ä –±–æ–ª–æ–Ω ”©–≥”©–≥–¥”©–ª –∞—á–∞–∞–ª–ª–∞—Ö
model = load_model("chatbot_model.h5")
intents = json.loads(open("job_intents.json", encoding="utf-8").read())
words = pickle.load(open("words.pkl", "rb"))
classes = pickle.load(open("classes.pkl", "rb"))

# üßπ –¢–µ–∫—Å—Ç —Ü—ç–≤—ç—Ä–ª—ç—Ö
def clean_up_sentence(sentence):
    sentence_words = wordpunct_tokenize(sentence)  # üÜï punkt –±–∏—à, wordpunct –∞—à–∏–≥–ª–∞–∂ –±–∞–π–Ω–∞
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words

# üß† Bag of Words
def bow(sentence, words, show_details=True):
    sentence_words = clean_up_sentence(sentence)
    bag = [0] * len(words)
    for s in sentence_words:
        for i, w in enumerate(words):
            if w == s:
                bag[i] = 1
                if show_details:
                    print(f"üìå Found in bag: {w}")
    return np.array(bag)

# üîç –ò–Ω—Ç–µ–Ω—Ç —Ç–∞–∞–º–∞–≥–ª–∞—Ö
def predict_class(sentence, model):
    p = bow(sentence, words, show_details=False)
    res = model.predict(np.array([p]))[0]
    ERROR_THRESHOLD = 0.4
    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]
    results.sort(key=lambda x: x[1], reverse=True)
    print("üîé Prediction Probabilities:", results)
    return [{"intent": classes[r[0]], "probability": str(r[1])} for r in results]

# üí¨ JSON-–æ–æ—Å —Ö–∞—Ä–∏—É –∞–≤–∞—Ö
def get_json_response(ints, intents_json):
    tag = ints[0]['intent']
    for i in intents_json['intents']:
        if i['tag'] == tag:
            return random.choice(i['responses'])

# üß† Chatbot —Ö–∞—Ä–∏—É
def chatbot_response(msg):
    print(f"üó£Ô∏è User Message: {msg}")
    ints = predict_class(msg, model)
    print(f"üß† Intent Prediction: {ints}")

    if ints:
        prob = float(ints[0]["probability"])
        if prob > 0.85:
            intent = ints[0]["intent"]
            log_user_question(msg, intent)
            return get_json_response(ints, intents)

    log_user_question(msg, "unknown")
    return "ü§ñ –£—É—á–ª–∞–∞—Ä–∞–π, —Ç–∞–Ω—ã –∞—Å—É—É–ª—Ç–∞–¥ —Ö–∞—Ä–∏—É–ª–∞—Ö –±–æ–ª–æ–º–∂–≥“Ø–π –±–∞–π–Ω–∞. –¢–∞ ”©”©—Ä”©”©—Ä –¥–∞—Ö–∏–Ω –æ—Ä–æ–ª–¥–æ–æ—Ä–æ–π."
